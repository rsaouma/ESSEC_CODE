{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Applied Supervised Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Case: Classifying Iris Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a start, we will explore a basic machine learning problem and create our first model. In the process, we will revist some core concepts and terms. For the sake of this particular case we would like to classify species of some iris flowers. We have collected some measurements associated with each iris: the length and width of the petals and the length and width of the sepals, all measured in centimeters(figure 1-2). We also have the measurements of some irises that have been previously identified by an expert botanist as belonging to the species setosa, versicolor, or virginica. For these measurements, we can be certain of which species each iris belongs to. Our goal is to build a machine learning model that can learn from the measurements of these irises whose species is known, so that we can predict the species for a new iris.\n",
    "\n",
    "MÃ¼ller, Andreas C.. Introduction to Machine Learning with Python . O'Reilly Media. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sepal_petal](sepal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we need to explore this case is built-in sklearn the machine learning package for python. We will first import that package then load the data as per below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meeting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wefefqrwegfqerg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn.utils.Bunch is data structure that is particular for sklearn. Since we are pandas experts the first thing we are going to do is to store that data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data= np.c_[iris_dataset['data'], iris_dataset['target']],\n",
    "                     columns= iris_dataset['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Success: Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step would be to split the data between training and testing sets. There is a build sklearn module that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the train_test_split function randomly split the data into a training set and a testing set. If we dont specify the split will be done on 75% (training) and 25%(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case about we decided to set the random state to 0. This is similar to setting a seed when dealing with python's random functions. We do that in this particular case in order for all of us to have the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from data in X_train\n",
    "# label the columns using the strings in iris_dataset.feature_names\n",
    "iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n",
    "# create a scatter matrix from the dataframe, color by y_train\n",
    "pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15),\n",
    "                           marker='o', hist_kwds={'bins': 20}, s=60,\n",
    "                           alpha=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Your First Model: k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[5, 2.9, 1, 0.2]])\n",
    "print(\"X_new.shape:\", X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = knn.predict(X_new)\n",
    "print(\"Prediction:\", prediction)\n",
    "print(\"Predicted target name:\",\n",
    "       iris_dataset['target_names'][prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n",
    "print(\"Test set predictions:\\n\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sensitive is k-NN classification accuracy to the train/test split proportion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for s in t:\n",
    "\n",
    "    scores = []\n",
    "    for i in range(1,1000):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'], iris_dataset['target'], test_size = 1-s)\n",
    "        knn.fit(X_train, y_train)\n",
    "        scores.append(knn.score(X_test, y_test))\n",
    "    plt.plot(s, np.mean(scores), 'bo')\n",
    "\n",
    "plt.xlabel('Training set proportion (%)')\n",
    "plt.ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Case: Classifying Fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruits = pd.read_table('fruit_data_with_colors.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from fruit label value to fruit name to make results easier to interpret\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   \n",
    "lookup_fruit_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = fruits[['height', 'width', 'mass', 'color_score']]\n",
    "y = fruits['fruit_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "cmap = cm.get_cmap('gnuplot')\n",
    "scatter = pd.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\n",
    "ax.set_xlabel('width')\n",
    "ax.set_ylabel('height')\n",
    "ax.set_zlabel('color_score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For this example, we use the mass, width, and height features of each fruit instance\n",
    "X = fruits[['mass', 'width', 'height']]\n",
    "y = fruits['fruit_label']\n",
    "\n",
    "# default is 75% / 25% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(1,20)\n",
    "scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('accuracy')\n",
    "plt.scatter(k_range, scores)\n",
    "plt.xticks([0,5,10,15,20]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating a synthetic data set using the built-in simulator of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for simple regression\n",
    "from sklearn.datasets import make_regression\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "X_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n",
    "                            n_informative=1, bias =250.0,\n",
    "                            noise = 30, random_state=0)\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After spliting the data into a training set and test set. We then try to fit a linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then Plot the regression line as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\n",
    "plt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\n",
    "plt.title('Least-squares linear regression')\n",
    "plt.xlabel('Feature value (x)')\n",
    "plt.ylabel('Target value (y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real data regression with crime data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime = pd.read_table('CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n",
    "# remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n",
    "columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n",
    "crime = crime.iloc[:,columns_to_keep].dropna()\n",
    "\n",
    "X_crime = crime.iloc[:,range(0,88)]\n",
    "y_crime = crime['ViolentCrimesPerPop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crime.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('linear model intercept: {}'\n",
    "     .format(linreg.intercept_))\n",
    "print('linear model coeff:\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results of R-squared the score is not great especially for the test data.\n",
    "Let's try another algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ridge we notice that the performance does not improve at all and the number of features remains 88."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('ridge regression linear model intercept: {}'\n",
    "     .format(linridge.intercept_))\n",
    "print('ridge regression linear model coeff:\\n{}'\n",
    "     .format(linridge.coef_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linridge.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linridge.score(X_test_scaled, y_test)))\n",
    "print('Number of non-zero features: {}'\n",
    "     .format(np.sum(linridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0, 1, 10, 20, 50, 100, 200]\n",
    "r2_train_l =[]\n",
    "r2_test_l =[]\n",
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in alphas:\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n",
    "    r2_train = linridge.score(X_train_scaled, y_train)\n",
    "    r2_train_l.append(r2_train)\n",
    "    r2_test = linridge.score(X_test_scaled, y_test)\n",
    "    r2_test_l.append(r2_test)\n",
    "    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "plt.plot(alphas,r2_test_l,label=\"Testing Performance\")\n",
    "plt.plot(alphas,r2_train_l,label=\"Training Performance\")\n",
    "plt.xlabel(\"Alpha =\")\n",
    "plt.ylabel(\"$R^2$ =\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n",
    "                                                   random_state = 0)\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "linlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Crime dataset')\n",
    "print('lasso regression linear model intercept: {}'\n",
    "     .format(linlasso.intercept_))\n",
    "print('lasso regression linear model coeff:\\n{}'\n",
    "     .format(linlasso.coef_))\n",
    "print('Non-zero features: {}'\n",
    "     .format(np.sum(linlasso.coef_ != 0)))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso.score(X_train_scaled, y_train)))\n",
    "print('R-squared score (test): {:.3f}\\n'\n",
    "     .format(linlasso.score(X_test_scaled, y_test)))\n",
    "print('Features with non-zero weight (sorted by absolute magnitude):')\n",
    "\n",
    "for e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n",
    "                key = lambda e: -abs(e[1])):\n",
    "    if e[1] != 0:\n",
    "        print('\\t{}, {:.3f}'.format(e[0], e[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization\\n\\\n",
    "parameter on number of features kept in final model\\n')\n",
    "\n",
    "for alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n",
    "    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n",
    "    r2_train = linlasso.score(X_train_scaled, y_train)\n",
    "    r2_test = linlasso.score(X_test_scaled, y_test)\n",
    "    \n",
    "    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, \\\n",
    "r-squared test: {:.2f}\\n'\n",
    "         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic dataset for more complex regression\n",
    "from sklearn.datasets import make_friedman1\n",
    "plt.figure()\n",
    "plt.title('Complex regression problem with one input variable')\n",
    "X_F1, y_F1 = make_friedman1(n_samples = 100,\n",
    "                           n_features = 7, random_state=0)\n",
    "\n",
    "plt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('linear model coeff (w): {}'\n",
    "     .format(linreg.coef_))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nNow we transform the original input data to add\\n\\\n",
    "polynomial features up to degree 2 (quadratic)\\n')\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_F1_poly = poly.fit_transform(X_F1)\n",
    "print(X_F1_poly.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2) R-squared score (test): {:.3f}\\n'\n",
    "     .format(linreg.score(X_test, y_test)))\n",
    "\n",
    "print('\\nAddition of many polynomial features often leads to\\n\\\n",
    "overfitting, so we often use polynomial features in combination\\n\\\n",
    "with regression that has a regularization penalty, like ridge\\n\\\n",
    "regression.\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,\n",
    "                                                   random_state = 0)\n",
    "linreg = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print('(poly deg 2 + ridge) linear model coeff (w):\\n{}'\n",
    "     .format(linreg.coef_))\n",
    "print('(poly deg 2 + ridge) linear model intercept (b): {:.3f}'\n",
    "     .format(linreg.intercept_))\n",
    "print('(poly deg 2 + ridge) R-squared score (training): {:.3f}'\n",
    "     .format(linreg.score(X_train, y_train)))\n",
    "print('(poly deg 2 + ridge) R-squared score (test): {:.3f}'\n",
    "     .format(linreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_ * X_F1_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Model for Asset Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factors models are reliably used in investement business and serve two main purposes.\n",
    "\n",
    "The first they reduce the complexity of modeling asset price movements.  For instance, trying to build a model that completely explains stock price movements is near impossible.  In order to build a model for your portfolio one would need to model supply, demand, sentiment, current and expected future earnings of the stock, news, interest rates, risk premia...\n",
    "\n",
    "It's near impossible to calibrate such a complicated model!  Instead, factor investors assume that there are N important factors that drive a portion of the asset returns.  They then say that at the portfolio level, asset specific movements can be averaged out, and only those N variables remain.  So to understand what drives the portfolio returns we only need to model the effect of that small number of factors.\n",
    "\n",
    "Alternativly, understanding the factor loadings of the individual assets allows us to estimate the covariance of our returns.  If one understands the factor loadings and the covariance of the factor returns, one can then compute an estimate for the covariance of the assets themselves.\n",
    "\n",
    "Finally factor models can also be used for hedging.  The factor loadings represent the hedging ratio one would use to minimize the volatility of your portfolio.\n",
    "\n",
    "In this module we will walk though multiple ways of estimating factor loadings, and discuss their relative strenghts and weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $y_t$ represent an asset return at time t, the linear factor model can help us interpret the source of the asset return and attribute it to the factor returns.\n",
    "\n",
    "In this example, we are interested in explaining the asset returns with a five-factor model:\n",
    "\n",
    "1) World Equity: This factor represents worldwide equity returns.\n",
    "\n",
    "2) US Treasury: This factor contains return from treasury bonds in United States, the bonds with the least risk.\n",
    "\n",
    "3) Bond Risk Premia: This is a credit factor that captures extra yield from risky bonds.  Defined as the spread between high risk bonds and US Treasury bonds.\n",
    "\n",
    "4) Inflation Protection: This is a \"style\" factor that considers the difference between real and nominal returns, thus balances the need for both.\n",
    "\n",
    "5) Currency Protection: This is also a \"style\" factor that includes risk premium for US domestic assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Examine the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_data = pd.read_csv(\"data_oct2018.csv\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['Date'] = pd.to_datetime(all_data['Date'])\n",
    "names = [\"Real Estate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = all_data.shape[0]\n",
    "totalReturns = np.zeros((n,len(names)))\n",
    "totalReturns[0,:] = 1.\n",
    "for i in range(1,n):\n",
    "    totalReturns[i,:] = np.multiply(totalReturns[i-1,:], (1+all_data[names].values[i,:]))\n",
    "for j in range(len(names)):\n",
    "    plt.semilogy(all_data['Date'], totalReturns[:,j])\n",
    "\n",
    "plt.title('Total Return Over Time')\n",
    "plt.ylabel('Total Return')\n",
    "plt.legend(names)\n",
    "plt.xlabel('Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we examine the worst dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.sort_values('US Equities').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "October 1987 was black monday and October / November 2008 was the beginnings of the financial crisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "ind_var=['World Equities','US Treasuries','Bond Risk Premium','Inflation Protection','Currency Protection']\n",
    "dep_var='Real Estate'\n",
    "X_fm = all_data[ind_var]\n",
    "y_fm = all_data[dep_var]\n",
    "linreg_FM = LinearRegression().fit(X_fm, y_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = linreg_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg_FM.score(X_fm, y_fm)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Drawbacks\n",
    "\n",
    "OLS has major drawbacks.  First, OLS has no mechanism to filter out noise variables.  Second, it assumes that factor loadings are constant over time. In practice, you will generally have many variables that you would like to filter down.  Moreover, the assumption that the factor loadings are constant over time is restrictive, and not true.  In fact, we will show that factor loadings are highly dependent on the time period.\n",
    "\n",
    "In this section we give example of the two drawbacks.  In section 3, we will introduce ways of filtering noise variables.\n",
    "\n",
    "To demonstrate how OLS can be susceptible to noise, we introduce a noise variable positivly correlated with the World Equities factor.  Then we re-run the OLS regression. The OLS regression chosses to average the two signals, changing the loading on the World Equity factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate a noise variable\n",
    "std = all_data['World Equities'].std()*(2)\n",
    "nPeriods = all_data.shape[0]\n",
    "np.random.seed(1235)\n",
    "noise = np.random.normal(loc=0, size=(nPeriods))\n",
    "noise = np.reshape(std*noise + np.array(all_data['World Equities']), (nPeriods,1))\n",
    "all_data['Noise'] = noise\n",
    "factorNameWithNoise = ['World Equities','US Treasuries','Bond Risk Premium','Inflation Protection','Currency Protection'] + ['Noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ind_var=factorNameWithNoise \n",
    "dep_var='Real Estate'\n",
    "X_fm = all_data[ind_var]\n",
    "y_fm = all_data[dep_var]\n",
    "linreg_FM = LinearRegression().fit(X_fm, y_fm)\n",
    "\n",
    "coef = linreg_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg_FM.score(X_fm, y_fm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the sum of World Equities Beta + Noise Beta is around .31.  Feel free to re-run the previous 2 code blocks.  It will generate a new noise term, and notice that each time the noise term will then effects the World Equities loading.  That noise term is directly causing estimation error in the true loading (Beta value) of the World Equity factor.\n",
    "To demonstrate the second drawback we will show that the OLS estimator depends greatly on the time period.  First, we will pick different time periods and run the OLS regression.  We'll show that the factor loadings can change dramatically depending on the time period.  To formalize this idea, we will filter our data into two different regimes.  The first regime \"normal\", will be months where US Equities had a positive monthly return.  The second, \"crash\" will be months where US Equities had a negative return.  These are crude approximations, but even with this crude definition we will return substantially different factor loadings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalData = all_data[all_data['US Equities'] > 0].copy()\n",
    "ind_var=['World Equities','US Treasuries','Bond Risk Premium','Inflation Protection','Currency Protection']\n",
    "dep_var='Real Estate'\n",
    "X_fm = normalData[ind_var]\n",
    "y_fm = normalData[dep_var]\n",
    "linreg_FM = LinearRegression().fit(X_fm, y_fm)\n",
    "\n",
    "coef = linreg_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg_FM.score(X_fm, y_fm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crashData = all_data[all_data['US Equities'] <= 0].copy()\n",
    "ind_var=['World Equities','US Treasuries','Bond Risk Premium','Inflation Protection','Currency Protection']\n",
    "dep_var='Real Estate'\n",
    "X_fm = crashData[ind_var]\n",
    "y_fm = crashData[dep_var]\n",
    "linreg_FM = LinearRegression().fit(X_fm, y_fm)\n",
    "\n",
    "coef = linreg_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linreg_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linreg_FM.score(X_fm, y_fm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with a crude definition of a \"crash\" regime we have isolated different factor loadings.  Notice that during normal periods the loading on Currency Protection is close to zero, but during crash periods is 10x larger!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Model with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "ind_var=['World Equities','US Treasuries','Bond Risk Premium','Inflation Protection','Currency Protection']\n",
    "dep_var='Real Estate'\n",
    "X_fm = all_data[ind_var]\n",
    "y_fm = all_data[dep_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linlasso_FM = Lasso(alpha=(0.0001),fit_intercept=True).fit(X_fm, y_fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coef = linlasso_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linlasso_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso_FM.score(X_fm, y_fm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_var=factorNameWithNoise \n",
    "dep_var='Real Estate'\n",
    "X_fm = all_data[ind_var]\n",
    "y_fm = all_data[dep_var]\n",
    "linlasso_FM = Lasso(alpha=(0.0002),fit_intercept=True).fit(X_fm, y_fm)\n",
    "\n",
    "coef = linlasso_FM.coef_\n",
    "for i in range(len(coef)):\n",
    "    print ( '{}, has a coefficient of {}'.format(ind_var[i],coef[i]))\n",
    "print('linear model intercept (b): {:.3f}'\n",
    "     .format(linlasso_FM.intercept_))\n",
    "print('R-squared score (training): {:.3f}'\n",
    "     .format(linlasso_FM.score(X_fm, y_fm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
